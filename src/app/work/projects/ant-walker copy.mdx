---
title: "Humanoid Agent : Apprentissage par renforcement sur Gymnasium MuJoCo"
publishedAt: "2025-11-25"
summary: "Agent IA entraîné pour contrôler un robot humanoïde bipède dans l’environnement Humanoid de Gymnasium MuJoCo"
images:
  - "/images/projects/Humanoide/humanoid_training.png"
  - "/images/projects/Humanoide/humanoid_walk.png"
  - "/images/projects/Humanoide/reward_curve.png"
team:
  - name: "Marine RAPIN"
    role: "AI / RL Researcher"
    avatar: "/images/avatar.jpg"
    linkedIn: "https://www.linkedin.com/in/marine-rapin-105095150//"
category: "ia-data"
---

## Description

Ce projet consiste à entraîner un agent pour contrôler un **humanoïde bipède** dans l’environnement **Humanoid** de **Gymnasium MuJoCo**, avec comme objectif de réaliser une locomotion stable et efficace. L’agent apprend à appliquer des torques aux articulations (hanches, genoux, bras) pour se tenir debout et marcher en avant, tout en minimisant le risque de chute.

L’environnement Humanoid est très complexe, car il simule un robot à 17 degrés d’action (torques) dans un espace d’observation très riche. :contentReference[oaicite:0]{index=0}  

## Objectifs

- Créer un agent capable de marcher de manière stable sans tomber  
- Maximiser la récompense cumulée liée à la vitesse tout en conservant l’équilibre  
- Étudier l’impact des architectures RL sur la locomotion humanoïde  
- Visualiser l’évolution des stratégies de l’agent (déplacements, posture, fréquence de chute)

## Fonctionnalités et Techniques

- **Apprentissage par renforcement profond** : utilisation d’un algorithme comme PPO, SAC ou DQN adapté à l’environnement Humanoid  
- **Contrôle des articulations** : les actions correspondent à des torques sur les articulations (17 dimensions) :contentReference[oaicite:1]{index=1}  
- **Replay Buffer / Target Network** : si c’est un algorithme de type Q‑learning, pour stabiliser l’apprentissage  
- **Exploration‑exploitation** : stratégie d’epsilon-greedy (ou autre) pour explorer suffisamment tout en exploitant les stratégies apprises  
- **Suivi des performances** : visualisation des courbes de récompense, taux de chute, distance parcourue, etc.  
- **Simulation physique avancée** : simulation de la physique du robot via MuJoCo (Gymnasium) :contentReference[oaicite:2]{index=2}  
- **Visualisation du comportement** : enregistrement des épisodes de simulation (vidéos) pour analyser le mouvement de l’agent

## Compétences Démontrées

- Conception et entraînement d’un agent RL dans un environnement **haute dimension**  
- Manipulation d’un environnement physique simulé (MuJoCo)  
- Optimisation d’architectures et réglage d’hyperparamètres pour maximiser la performance  
- Visualisation et interprétation des trajectoires et des comportements émergents  
- Gestion de la stabilité et de l’équilibre en contrôle continu

## Technologies Utilisées

- **Python 3.x**  
- **Gymnasium** avec l’environnement **Humanoid‑v4 ou v5** :contentReference[oaicite:3]{index=3}  
- **MuJoCo** pour la simulation physique :contentReference[oaicite:4]{index=4}  
- **PyTorch** (ou TensorFlow) pour le réseau neuronal de l’agent  
- Bibliothèques de visualisation : **Matplotlib**, **Seaborn**  
- Outils d’entraînement RL : possibilité d’utiliser **Stable‑Baselines3**, **RLlib**, ou une implémentation custom

---